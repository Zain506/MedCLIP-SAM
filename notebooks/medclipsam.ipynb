{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOb/huXUVt18VOqnRuNu7Wx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain506/MedCLIP-SAM/blob/main/notebooks/medclipsam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedCLIP-SAM\n",
        "\n",
        "[Research Paper](https://arxiv.org/pdf/2403.20253)\n",
        "\n"
      ],
      "metadata": {
        "id": "YdgyzEvf-BEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2f1L8ESy99up"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"adishourya/MEDPIX-ClinQA\") # Import MedPIX dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_valid = ds[\"train\"].train_test_split(test_size=0.15)\n",
        "training = train_valid[\"train\"] # .select(range(1024))\n",
        "validation = train_valid[\"test\"]\n",
        "print(training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4UcdOTutWLR",
        "outputId": "093a44d6-9aed-4e47-caa3-ae8df5bfbffe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['image_id', 'mode', 'case_id', 'question', 'answer'],\n",
            "    num_rows: 17425\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Processor: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nvrQtXqdwFr",
        "outputId": "f4e51fc7-131c-471a-bf77-f30779f5869d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are 3 components of the CLIP-style architecture\n",
        "1. Text Encoder (PubMedBERT)\n",
        "2. Image Encoder (ViT)\n",
        "3. Model mapping encoded images and text into the shared embedding space\n",
        "\n",
        "**BiomedCLIP from open_clip contains all of them, and that is what MedCLIP-SAM aims to fine-tune**"
      ],
      "metadata": {
        "id": "ECwEYsPocsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap\n",
        "%pip install open_clip_torch # run ``cap.show()`` in order to see the output here"
      ],
      "metadata": {
        "id": "RkuVklKXf8eu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "# model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
      ],
      "metadata": {
        "id": "ui9m6RPEf0Ah"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "B: int = 128 # Batch Size\n",
        "# In this case the data in the dataset isn't in tensor form - so we need to define a custom collate_fn to convert them into tensors\n",
        "def collate_fn(batch): # Define custom collate_fn to convert relevant data in each batch to tensor\n",
        "  images = torch.stack([preprocess(x['image_id']) for x in batch])\n",
        "  texts = tokenizer([f\"Prompt: {x['question']} \\nAnswer: {x['answer']}\" for x in batch])\n",
        "\n",
        "  return images, texts\n",
        "\n",
        "train_loader = DataLoader(training, batch_size=B, shuffle=True, collate_fn=collate_fn) # Each tensor in the train_loader will have dimension `batch_size` in axis=0"
      ],
      "metadata": {
        "id": "R-qzEDOwib1-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "def calcLoss(A: torch.Tensor): # Define loss function here\n",
        "  if A.shape != (B, B):\n",
        "    return \"Invalid shape\"\n",
        "  X = A.clone().to(device)\n",
        "  I = torch.eye(B).to(device)\n",
        "  X = X - I @ X\n",
        "  W = ((B-1) * F.softmax(X, dim=-1)) - I\n",
        "  loss = torch.sum(torch.log(torch.sum(A*W, dim=-1)))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "72tnJz4plhBA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over images, texts in train_loader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "optimizer = torch.optim.AdamW( # Freezing a part of the model requires us to set requires_grad=False\n",
        "    model.parameters(), # All trainable params\n",
        "    lr = 5e-6,\n",
        "    weight_decay = 0.01 # L1 reg\n",
        ")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  for images, texts in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "    imgs = model.encode_image(images) # I_{p, i}\n",
        "    txts = model.encode_text(texts) # T_{p, i}\n",
        "    imgs = F.normalize(imgs, p=2, dim=-1) # Normalise each vector\n",
        "    txts = F.normalize(txts, p=2, dim=-1)\n",
        "\n",
        "\n",
        "    B = imgs.size(0)\n",
        "    t = 0.7 # temperature\n",
        "    sim = imgs @ txts.T\n",
        "    # print(sim.shape) # Should be batch size x batch size\n",
        "    # print(imgs.device)\n",
        "    # print(txts.device)\n",
        "    # print(sim.device)\n",
        "    # print(next(model.parameters()).device)\n",
        "    # Calculate loss here\n",
        "    A = torch.exp(sim / t)\n",
        "    loss = calcLoss(A) + calcLoss(A.t()) # Decouple Hard Negative Noise Contrastive Estimation\n",
        "    # print(loss)\n",
        "    # Backpropagate here\n",
        "    optimizer.zero_grad() # Set gradient for model parameters to 0 before computing new ones\n",
        "    loss.backward() # Calculates derivatives\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "52dFCU-P0uzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}