{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZPbWvQUuvhanl+FcQk2lT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain506/MedCLIP-SAM/blob/main/notebooks/medclipsam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedCLIP-SAM\n",
        "\n",
        "[Research Paper](https://arxiv.org/pdf/2403.20253)\n",
        "\n"
      ],
      "metadata": {
        "id": "YdgyzEvf-BEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f1L8ESy99up"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"adishourya/MEDPIX-ClinQA\") # Import MedPIX dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_valid = ds[\"train\"].train_test_split(test_size=0.15)\n",
        "training = train_valid[\"train\"] # .select(range(1024))\n",
        "validation = train_valid[\"test\"]\n",
        "print(training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4UcdOTutWLR",
        "outputId": "f94075a0-5da7-440f-8aee-2e91a7a19ecb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['image_id', 'mode', 'case_id', 'question', 'answer'],\n",
            "    num_rows: 17425\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Processor: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nvrQtXqdwFr",
        "outputId": "c11d3026-19ac-4aaa-a575-a6bf9edd6673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are 3 components of the CLIP-style architecture\n",
        "1. Text Encoder (PubMedBERT)\n",
        "2. Image Encoder (ViT)\n",
        "3. Model mapping encoded images and text into the shared embedding space\n",
        "\n",
        "**BiomedCLIP from open_clip contains all of them, and that is what MedCLIP-SAM aims to fine-tune**"
      ],
      "metadata": {
        "id": "ECwEYsPocsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap\n",
        "%pip install open_clip_torch # run ``cap.show()`` in order to see the output here"
      ],
      "metadata": {
        "id": "RkuVklKXf8eu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "# model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "model = model.to(device)\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
      ],
      "metadata": {
        "id": "ui9m6RPEf0Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "bsize: int = 10 # Batch Size\n",
        "# In this case the data in the dataset isn't in tensor form - so we need to define a custom collate_fn to convert them into tensors\n",
        "def collate_fn(batch): # Define custom collate_fn to convert relevant data in each batch to tensor\n",
        "  images = torch.stack([preprocess(x['image_id']) for x in batch])\n",
        "  texts = tokenizer([f\"Prompt: {x['question']} \\nAnswer: {x['answer']}\" for x in batch])\n",
        "\n",
        "  return images, texts\n",
        "\n",
        "train_loader = DataLoader(training, batch_size=bsize, shuffle=True, collate_fn=collate_fn) # Each tensor in the train_loader will have dimension `batch_size` in axis=0"
      ],
      "metadata": {
        "id": "R-qzEDOwib1-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def calcLoss(sim: torch.Tensor, b: float) -> torch.Tensor: # Loss found in Proof.pdf\n",
        "  A = torch.exp(sim)\n",
        "  X = b1*sim\n",
        "  X.diagonal().fill_(-torch.inf)\n",
        "  W = (bsize-1)*torch.softmax(X, dim=-1)\n",
        "  W.diagonal().fill_(-1)\n",
        "  loss = torch.sum(torch.log(torch.sum(A*W, dim=-1)))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "KDWSgN9pA-FD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over images, texts in train_loader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "optimizer = torch.optim.AdamW( # Freezing a part of the model requires us to set requires_grad=False\n",
        "    model.parameters(), # All trainable params\n",
        "    lr = 5e-6,\n",
        "    weight_decay = 0.01 # L1 reg\n",
        ")\n",
        "\n",
        "t = 0.1\n",
        "b1 = 0.1\n",
        "b2 = 0.1\n",
        "\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  for images, texts in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "    I = model.encode_image(images)\n",
        "    I = F.normalize(I, p=2, dim=-1)\n",
        "    T = model.encode_text(texts)\n",
        "    T = F.normalize(T, p=2, dim=-1)\n",
        "\n",
        "    sim = I @ T.T\n",
        "    sim /= t\n",
        "\n",
        "    loss = calcLoss(sim, b1) + calcLoss(sim.T, b2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "52dFCU-P0uzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}