{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyJFxzHOW+Qzv1sPLbZsa6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zain506/Similarity/blob/main/notebooks/medclipsam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedCLIP-SAM\n",
        "\n",
        "[Research Paper](https://arxiv.org/pdf/2403.20253)\n",
        "\n"
      ],
      "metadata": {
        "id": "YdgyzEvf-BEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2f1L8ESy99up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd4ae43-75c3-46a0-ec24-9613430d4208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image_id', 'mode', 'case_id', 'question', 'answer'],\n",
            "        num_rows: 20500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"adishourya/MEDPIX-ClinQA\") # Import MedPIX dataset\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Processor: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nvrQtXqdwFr",
        "outputId": "c1123737-93f6-4745-a86a-84d9e97553da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are 3 components of the CLIP-style architecture\n",
        "1. Text Encoder (PubMedBERT)\n",
        "2. Image Encoder (ViT)\n",
        "3. Model mapping encoded images and text into the shared embedding space\n",
        "\n",
        "**BiomedCLIP from open_clip contains all of them, and that is what MedCLIP-SAM aims to fine-tune**"
      ],
      "metadata": {
        "id": "ECwEYsPocsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap\n",
        "%pip install open_clip_torch # run ``cap.show()`` in order to see the output here"
      ],
      "metadata": {
        "id": "RkuVklKXf8eu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "\n",
        "# This section should handle a batch and construct the similarity matrix bit by bit\n",
        "image = preprocess(ds[\"train\"][0][\"image_id\"]).unsqueeze(0)\n",
        "text = tokenizer([\"Subcranial Hematoma\", \"Intracranial Hematoma\", \"Perfectly healthy brain\"])\n",
        "\n",
        "with torch.no_grad(), torch.autocast(\"cuda\"):\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui9m6RPEf0Ah",
        "outputId": "b8b9a84e-c92d-4785-efde-c093fa324210"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: tensor([[3.7976e-02, 9.6202e-01, 5.6641e-06]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkZ62eDahHNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}